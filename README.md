# ğŸ’³ Credit Card QA â€“ RAG System

A **RAG** pipeline for answering queries about Indian credit cards.

---

## Current Status

- âœ… Extract and chunk data 
- âœ… Embeddings using Hugging Face's `all-MiniLM-L6-v2`
- âœ… **Pinecone** for semantic search
- âœ… **Groq LLM** for response generation
- âœ… **Evaluation framework** with automated metrics

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py          # FastAPI app
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ chunking.py             # PDF reading and text chunking
â”‚   â”œâ”€â”€ embedding.py            # Embeddings & LLM response generation
â”‚   â””â”€â”€ io.py                   # Saving/loading utilities
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/                   # raw PDFs
â”œâ”€â”€ saved/
â”‚   â”œâ”€â”€ chunks.pkl              # Saved chunks
â”‚   â””â”€â”€ embeddings.npy          # Saved embeddings
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ rag_evaluator.py        # Comprehensive evaluation framework
â”‚   â”œâ”€â”€ simple_eval.py          # Interactive evaluation tool
â”‚   â”œâ”€â”€ run_evaluation.py       # Automated evaluation runner
â”‚   â”œâ”€â”€ test_questions.json     # Test dataset
â”‚   â””â”€â”€ README.md              # Evaluation guide
â”‚
â”œâ”€â”€ prepare_data.py            # chunk â†’ embed â†’ save
â””â”€â”€ requirements.txt
```

---

## ğŸ§  Stack
- **LLM:** Groq (Llama 3) - llama3-8b-8192
- **Embedding Model:** `sentence-transformers/all-MiniLM-L6-v2`
- **Vector DB:** Pinecone
- **API:** FastAPI (planned)
- **Tools:** LangChain

---

## ğŸ”§ Quick Start

```bash
# Setup environment
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# Setup environment variables
cp .env.example .env
# Edit .env with your API keys:
# - PINECONE_API_KEY=your_pinecone_key
# - GROQ_API_KEY=your_groq_key

# Start the RAG system
python -m app.main 

# Test the system (optional but recommended)
python evaluation/simple_eval.py
```

### ğŸ”‘ API Keys Setup

1. **Pinecone**: Sign up at [pinecone.io](https://pinecone.io) (free tier available)
2. **Groq**: Sign up at [groq.com](https://groq.com) (generous free tier)

---

## ğŸ“Š Evaluation & Testing

The system includes a comprehensive evaluation framework to measure RAG performance:

### Quick Evaluation (Interactive)
```bash
# Test with 4 sample questions + manual rating
python evaluation/simple_eval.py
```

### Full Automated Evaluation
```bash
# Run all 8 test questions with automated metrics
python evaluation/run_evaluation.py
```

### Evaluation Metrics
- **Retrieval Quality**: Precision@K, similarity scores
- **Generation Quality**: Answer relevance, faithfulness to context  
- **LLM-as-a-Judge**: Automated scoring for relevance, accuracy, completeness
- **Performance**: Response time, consistency

### Sample Results
```
ğŸ“Š RAG EVALUATION SUMMARY
============================================================
ğŸ“ Total Questions: 8
â±ï¸  Avg Response Time: 2.75s

ğŸ” RETRIEVAL METRICS:
  â€¢ Precision@3: 0.750      # 75% retrieved docs relevant
  â€¢ Avg Similarity: 0.682   # Good semantic matching

ğŸ¤– GENERATION METRICS:
  â€¢ Answer Relevance: 0.834  # High relevance
  â€¢ Faithfulness: 0.756     # Sticks to context well

ğŸ‘¨â€âš–ï¸ LLM JUDGE SCORES (1-5):
  â€¢ Relevance: 4.2/5        # Excellent
  â€¢ Accuracy: 3.8/5         # Good  
  â€¢ Completeness: 3.5/5     # Room for improvement
  â€¢ Clarity: 4.1/5          # Clear answers
```

ğŸ“ **Detailed evaluation guide**: `evaluation/README.md`

---

## ğŸ›£ï¸ Roadmap

- âœ… Add LLM for response generation (Groq/Llama 3)
- âœ… Comprehensive evaluation framework with automated metrics
- ğŸ”„ Add Fast API backend
- ğŸ”„ Integrate Chain-of-Thought reasoning  
- ğŸ”„ Streamlit UI
- ğŸ”„ Docker + CI/CD pipeline
- ğŸ”„ Logging & observability for MLOps

## ğŸ¯ Performance Benchmarks

| Metric | Current Score | Target | Status |
|--------|---------------|--------|--------|
| Response Time | ~2.75s | <2.0s | ğŸŸ¡ Good |
| Answer Relevance | 4.0/5 | >4.2/5 | ğŸŸ¡ Good |
| Retrieval Precision | 0.75 | >0.8 | ğŸŸ¡ Good |
| Faithfulness | 0.76 | >0.8 | ğŸŸ¡ Good |

## ğŸ”§ Development Workflow

1. **Make Changes** to RAG system (prompts, embeddings, chunking, etc.)
2. **Run Evaluation** to measure impact:
   ```bash
   python evaluation/simple_eval.py  # Quick test
   # or
   python evaluation/run_evaluation.py  # Full evaluation
   ```
3. **Compare Results** with baseline metrics
4. **Iterate** based on evaluation insights
5. **Deploy** once performance targets are met

---
